---
title: "Variable Selection Using the rfUtiltities Package in R"
author: "Dan Carver"
date: "2018/04/06"
output:
  html_document:
    highlight: tango
    number_sections: yes
    theme: journal
    toc: yes
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, warning = FALSE, max.rows=10)
```
# Variable Selection
In the world of ecological systems everything is connected. As a result, there are lots and lots of variables or predictors that can be used to defined any given systes. In a perfect world if we wanted to model ecological conditions we would compile all the factors that influence that system and feed it all into our model. Luckly for us our world is not perfect.
A better way to look at the modelling process is to pratice the **priciple of parsimony**. Which is essential explain the system as well as you can with the fewest number of variables. This is where expect opinion comes in handy. If your trying to predict the presence of a specific tree species knowing it's ecological range and ideal growing conditions will allow you to define what predictor variables make the most sense for your model. Yet sometimes you might be tasked with modelling a system where those constriants are not well known or the data that represents those limits are not contained in available datasets. At this point using a computer to select the ideal variables for you is a trusted method that will give your work a credible foot to stand on.  

## Setting the Stage
Let's get started by ensuring all our ducks are in a row. Below is a listing of all the packages that are used by R to complete this analysis.

```{r}
# this is your working directory, it shows where all the files will be stored
getwd()

```
## Pulling in and examing predictor variables
For this example were going to look at a dataset that consists of remotely sensed reflectance values for various geographic locations where a species is either presence or absence. To run our variable selection process we will need to define our responce variable and predictor variables. Our variable selection process will tell us which variable best predict the responce variable. Let's pull it into R and look at what we are working with.

Open the csv in excel and check out the data before bringing it into R

```{r}

# import the csv as a data frame
data <- read.csv('testData1.csv')

# get an idea of the size of the data frame
rows <- nrow(data)
columns <- ncol(data)
print(paste("This data frame consists of ",rows," rows and ",columns," columns."))
```
That's quite a bit of data. Printing out the whole thing would be a bit of a mess and hard to look at. Let's limit some of the information.
```{r fig.cap = "Example of the structure of the data that will be used within this analysis."}
# use indexing and the head function to print only the first 5 rows and the first 10 columns
head(data[,1:10])

# check the end of the dataset as well
colnames(data[125:135])
```
Okay, that's better. We see some stadard information such as ID, X and Y coordinates, and year. Our last variable is "X0415_blue", which refers to the blue band of a Landsat image (specifically April 2015). We will be able to use the 'PercentAb' as our responce variables and all the remotely sensed data as the predictor variables.


## Variable Selection Process
We currently have 125 predictor variables available for us within this dataset, which is awesome. On the other hand, we can expect that many of these variables are correlated and that the run time of these models increases with the number of predictor variables.
The goal of any model should be to find the right number of variables to describe the system, no more, no less.

So we're going to conduct some variable selection to limit down the number of predictors.
There are many ways to perform a variable selection. The two below are options that we has tested;
[RFUtilities](https://www.rdocumentation.org/packages/rfUtilities/versions/2.1-0)
and
[VSURF](https://www.rdocumentation.org/packages/VSURF/versions/1.0.3/topics/VSURF).
Both have their pros and cons. RFutilities is fast and commonly used throughout the literature. VSURF is a more robust process that produced variables that led to higher model predictions in our study.
For this example we're going to use the the variable selection function of rfUtilities.
If your doing this on your own it's import to have some justification to the which selection process you choose. So read up on the process, think about it, read it again, and hopefully something has stuck by then. These functions are not the most intuitive processes so be prepared to spend some time with the documentation if you really need to understand what is happening in the black box.   

```{r fig.cap = "List of the top ten predictors and there relative importance to the model. These are taken from 125 potential predictors."}

#import Rf utilitties
install.packages("rfUtilities", "randomForest", "dplyr")
library(rfUtilities)
library(dplyr)
```

```{r}
# Set random seed
set.seed(123)

# Create a new data frame with only the raster data
rastercolumns <- data %>% 
  select(10:135)
#Run the rf utilities fuction to rank the variable in order of importance
varimportance_cov <- rf.modelSel(rastercolumns, data$Percent_Cov, imp.scale="se")

#create a dataframe with the rownames(predictor names) and thier relative importance
#this willproduce one column of variable names and one column of values
varimportance_cov <- cbind(rownames(varimportance_cov$sel.importance), varimportance_cov$sel.importance)
# Delete the row names because we've tied them to our dataframe already
rownames(varimportance_cov) <- NULL
# define the column names for indexing later
colnames(varimportance_cov) <- c("name", "importance")

# create a new dataframe where the mostimportant variable is listed at the top
varimportance_cov_ord <- varimportance_cov %>%
  arrange(desc(varimportance_cov$imp))
# take a look at the top 15 predictor variables
glimpse(varimportance_cov_ord)

```
Alright, so we now have a ranked list of variables that is based on the rfUtilities variable selection tool. The variable that have to the highest predictive capabilities are at the top. At this point we have not remove any variable. Before we move on to the next step you should decide how many variables it makes sence to keep. We have a list of the relative importance of all predictor so you can see which ones are really the big players. We would suggest including at the most 15 predictor variables within the next step. In the example below we will just be looking at the top ten.


## Removing Correlated Variables
Recursive models such as Random Forest handle correlated variables rather well, yet it is an established modeling technique to reduce the number of correlated variables.
We will do this by inspecting a correlation matrix and deciding what stays and what goes. Ideally this decision process is informed by both correlation values and knowledge of the importance of the predictor in capturing ecological conditions. If you have that expert knowlegde, use it. If not, just follow the framework we've established below.

```{r}
install.packages('corrplot')
library(corrplot)
```

```{r fig.cap = "A correlation plot between the ten top predictors; a Pearsons correlation coefficient of 0.7 was used."}

# drop columns that are not as important based on rfmodelsel
raster_cov_names <- varimportance_cov_ord %>%
  slice(1:10)

#select the predictor variables from the oringinal data set the correspond to the
# top ten predictors from our variable selection
rastercolumns_cov <- rastercolumns %>%
  select(c(raster_cov_names$name))
# Calculate correlation coefficient matrix
correlation <-cor(rastercolumns_cov, method="pearson")

# Plot the correlation. The darker the number,
# the more correlated the two variables
corrplot(correlation,method="number")

###
# see about adapting this to a ggplot2 structure for display #http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
##3

```

This correlation plot shows how the top ten prediction variables are correlated with one another. We're going to work at removing relationships where variables are correlated at values greater then 70 percent. We will favor the variables that have more predictive power. Below is a step by step process as to how we slimmed down number of variables.



1. Start with the first row. Note the position of any variable with >|0.7| correlation
    + No correlations present
2. Move to the second row and repeat
    + 5
    + 9
    + 10
3. Move to the thrid column and repeat
    + 6
    + 7

At this point it becomes difficult to remember which columns have been removed. Let's rerun our correlation plot removing the correlated indicies we've already identified (5,9,10,4,7)

```{r fig.cap = "A second correlation plot using a subset of the original top ten predictors. Producing multiple correlation plots is advisable to attempt to decipher large correlation plots."}

# We're using indexing to remove the rows that correspond to the correlated predictor variables
rastercolumns_cov2 <- rastercolumns_cov %>%
  select(-5,-9,-10,-6,-7)

# Calculate correlation coefficient matrix
correlation2 <-cor(rastercolumns_cov2, method="pearson")

# Plot the correlation. The darker the number, the more correlated the two
# variables
corrplot(correlation2, method="number")
```
The new correlation plot shows that there are 5 variables that can be used in the model, which are all significant and important.
It's good to package this up into a usable dataframe going forward.

```{r}
#create a new dataframe with the presence absence value (named species_PA) and
# the final predictors that you will using going forward in the modeling process
selected_binary <- rastercolumns_cov2 %>%
  mutate(species_cov = data$Percent_Cov)
```


### Alternative route
The example above works well but it's a little clumsy and takes a bit of pen and paper work to make sure your getting it all right.
The process belows allows you to accomplish the same thing but it's all down within the code editor. Go with whatever method works well for you.


```{r}

rastercolumns_cov <- rastercolumns %>%
  select(c(raster_cov_names$name))

# Calculate correlation coefficient matrix
correlation <-cor(rastercolumns_cov, method="pearson")

# Plot the correlation. The darker the number,
# the more correlated the two variables
corrplot(correlation,method="number")

# the read line funtion will show up in your code editors console. You will have to type the numbers in there following the specific format defined below.
x1 <- readline("Enter the numbers that you would like to remove seperated by commas. eg (4,25,3)")
x1 <- as.numeric(unlist(strsplit(x1, ",")))

select_var1 <- rastercolumns_cov[,-c(x1)]

correlation <-cor(select_var1, method="pearson")
corrplot(correlation,method="number")

x2 <- readline("Enter the numbers that you would like to remove seperated by commas. eg (4,25,3).")
x2 <- as.numeric(unlist(strsplit(x2, ",")))

select_var2 <- select_var1[,-c(x2)]

correlation <-cor(select_var2, method="pearson")
corrplot(correlation,method="number")

#If you do not need to remove values at this point just alter your code at the
# bottom to only include the selected variables you want.
### examples
#  selected_binary <- cbind(species_PA = Data$PresenceAb,  select_var1)
#  selected_binary <- cbind(species_PA = Data$PresenceAb,  select_var2)
#  selected_binary <- cbind(species_PA = Data$PresenceAb,  select_var3)
###
# Likewise you can add more steps to this process by repeating this process

x3 <- readline("Enter the numbers that you would like to remove seperated by commas. eg (4,25,3).")
x3 <- as.numeric(unlist(strsplit(x3, ",")))

select_var3 <- select_var2[,-c(x3)]

correlation <-cor(select_var3, method="pearson")
corrplot(correlation,method="number")

#at this point there are 8 variables that are not correlated binded them to the binary indence and put into rf model
selected_binary <- cbind(species_PA = data$Percent_Cov,  select_var1)

```
## Wrapping It Up
Variable selection is import and it's a good process to spend time thinking about.
While computers are good at what they do you should all ways be asking if the results
make sence to your specific project. These tools can provide justification for
your results but they should never be the only reason you make your choices.
